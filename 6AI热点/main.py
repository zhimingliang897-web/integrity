import os
import json
import requests
import feedparser
from openai import OpenAI
from datetime import datetime, timezone, timedelta

# --- é…ç½® ---
QWEN_API_KEY = os.getenv("QWEN_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "-1003878273027")
MODEL_NAME = "qwen3-vl-flash-2026-01-22"
TARGET_COUNT = 10  # ç›®æ ‡æ¨é€æ¡æ•°

client = OpenAI(
    api_key=QWEN_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# --- ä¿¡æºåˆ—è¡¨ ---
SOURCES = [
    # ===== å›½é™…ç¡¬æ ¸ =====
    {
        "name": "Hacker News",
        "url": "https://hnrss.org/newest?q=AI+LLM+Transformer+GPT",
        "type": "general",
        "count": 8,
    },
    {
        "name": "HF Papers",
        "url": "https://rsshub.app/huggingface/daily-papers",
        "type": "general",
        "count": 8,
    },
    {
        "name": "GitHub Trending",
        "url": "https://rsshub.app/github/trending/daily/python?since=daily",
        "type": "general",
        "count": 6,
    },
    {
        "name": "ArXiv cs.AI",
        "url": "https://rss.arxiv.org/rss/cs.AI",
        "type": "general",
        "count": 6,
    },
    {
        "name": "ArXiv cs.CL",
        "url": "https://rss.arxiv.org/rss/cs.CL",
        "type": "general",
        "count": 6,
    },
    # ===== å›½é™…ç¤¾åŒº =====
    {
        "name": "Reddit LocalLLaMA",
        "url": "https://www.reddit.com/r/LocalLLaMA/.rss",
        "type": "general",
        "count": 6,
    },
    {
        "name": "Reddit MachineLearning",
        "url": "https://www.reddit.com/r/MachineLearning/.rss",
        "type": "general",
        "count": 6,
    },
    {
        "name": "OpenAI Blog",
        "url": "https://rsshub.app/openai/blog",
        "type": "general",
        "count": 5,
    },
    {
        "name": "Google AI Blog",
        "url": "https://rsshub.app/google/research",
        "type": "general",
        "count": 5,
    },
    {
        "name": "MIT Tech Review AI",
        "url": "https://www.technologyreview.com/feed/",
        "type": "general",
        "count": 5,
    },
    # ===== å›½å†…æƒå¨ =====
    {
        "name": "æœºå™¨ä¹‹å¿ƒ",
        "url": "https://www.jiqizhixin.com/rss",
        "type": "cn_media",
        "count": 6,
    },
    {
        "name": "é‡å­ä½",
        "url": "https://rsshub.app/qbitai/category/èµ„è®¯",
        "type": "cn_media",
        "count": 6,
    },
]


def call_qwen(prompt: str) -> str | None:
    """è°ƒç”¨ Qwen API (OpenAI å…¼å®¹æ¥å£)"""
    try:
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
        )
        return resp.choices[0].message.content
    except Exception as e:
        print(f"[Qwen API Error] {e}")
        return None


def analyze_news(title: str, summary: str, source_type: str) -> dict | None:
    """ä½¿ç”¨ LLM å¯¹å•æ¡æ–°é—»è¯„çº§ï¼Œè¿”å› dict æˆ– None"""
    anti_hype = ""
    if source_type == "cn_media":
        anti_hype = (
            "æ³¨æ„ï¼šæœ¬æ–‡æ¥è‡ªä¸­æ–‡è‡ªåª’ä½“ï¼Œè¯·é€‚å½“å¿½ç•¥'ç‚¸è£‚''é¢ è¦†''ç¥ä½œ'ç­‰è¥é”€è¯æ±‡ï¼Œ"
            "ä¾§é‡å…³æ³¨æ˜¯å¦æœ‰ä»£ç /è®ºæ–‡/å®è´¨æŠ€æœ¯å†…å®¹ã€‚ä½†å¦‚æœåº•å±‚å†…å®¹ç¡®å®æœ‰ä»·å€¼ï¼Œä¸è¦å› ä¸ºæ ‡é¢˜å¤¸å¼ å°±é™åˆ†ã€‚"
        )

    prompt = f"""ä½ æ˜¯ä¸€ä½ AI æŠ€æœ¯æƒ…æŠ¥åˆ†æå¸ˆï¼Œè´Ÿè´£ä¸ºæŠ€æœ¯ä»ä¸šè€…ç­›é€‰æ¯æ—¥å€¼å¾—å…³æ³¨çš„å†…å®¹ã€‚
{anti_hype}

è¯·åˆ†æä»¥ä¸‹æ–°é—»ï¼š
æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{summary}

è¯„çº§æ ‡å‡†ï¼ˆè¯·åˆç†ç»™åˆ†ï¼Œä¸è¦è¿‡äºè‹›åˆ»ï¼‰ï¼š
- Sçº§ (9-10): è¡Œä¸šé‡Œç¨‹ç¢‘äº‹ä»¶ã€é¢ è¦†æ€§æ¶æ„çªç ´ã€é‡é‡çº§äº§å“å‘å¸ƒã€‚
- Açº§ (7-8): é«˜è´¨é‡è®ºæ–‡ã€å®ç”¨å·¥å…·é‡å¤§æ›´æ–°ã€å¤§å‚é‡è¦å¼€æºã€å€¼å¾—å…³æ³¨çš„è¡Œä¸šåŠ¨æ€ã€‚
- Bçº§ (5-6): æœ‰ä¸€å®šå‚è€ƒä»·å€¼çš„å†…å®¹ã€ä¸­ç­‰è´¨é‡çš„æŠ€æœ¯åˆ†äº«ã€æ™®é€šäº§å“è¿­ä»£ã€‚
- Cçº§ (1-4): çº¯æ°´æ–‡ã€æ— å®è´¨å†…å®¹çš„å…¬å…³ç¨¿ã€é‡å¤æ—§é—»ã€‚

è¯·åªè¾“å‡ºä¸€ä¸ªåˆæ³• JSONï¼Œä¸è¦åŒ…å« Markdown ä»£ç å—æ ‡è®°ï¼š
{{"rating":"S/A/B/C","score":æ•´æ•°1åˆ°10,"comment":"ä¸€å¥è¯ç‚¹è¯„(ä¸­æ–‡,30å­—å†…)","tags":["æ ‡ç­¾1","æ ‡ç­¾2"]}}"""

    text = call_qwen(prompt)
    if not text:
        return None

    clean = text.replace("```json", "").replace("```", "").strip()
    start = clean.find("{")
    end = clean.rfind("}") + 1
    if start == -1 or end == 0:
        print(f"[JSON æå–å¤±è´¥] {title}")
        return None
    try:
        return json.loads(clean[start:end])
    except json.JSONDecodeError:
        print(f"[JSON è§£æå¤±è´¥] {title}")
        return None


def fetch_source(source: dict) -> list[dict]:
    """æŠ“å–å•ä¸ª RSS ä¿¡æºå¹¶è¯„çº§ï¼Œè¿”å›å¸¦è¯„åˆ†çš„æ¡ç›®åˆ—è¡¨"""
    name = source["name"]
    count = source.get("count", 5)
    print(f">> æ­£åœ¨æŠ“å–: {name} ...")
    try:
        feed = feedparser.parse(source["url"])
    except Exception as e:
        print(f"[RSS æŠ“å–å¤±è´¥] {name}: {e}")
        return []

    if not feed.entries:
        print(f"   (æ— å†…å®¹)")
        return []

    results = []
    for entry in feed.entries[:count]:
        title = getattr(entry, "title", "").strip()
        summary = getattr(entry, "summary", "")[:800]
        link = getattr(entry, "link", "")
        if not title:
            continue

        analysis = analyze_news(title, summary, source["type"])
        if not analysis:
            continue

        rating = analysis.get("rating", "C")
        score = analysis.get("score", 0)
        results.append(
            {
                "source": name,
                "title": title,
                "link": link,
                "rating": rating,
                "score": score,
                "comment": analysis.get("comment", ""),
                "tags": analysis.get("tags", []),
            }
        )
        print(f"   [{rating}|{score}] {title}")

    return results


def select_top_news(all_news: list[dict]) -> list[dict]:
    """æŒ‰è¯„åˆ†æ’åºï¼Œä¼˜å…ˆ S/Aï¼Œä¸å¤Ÿåˆ™è¡¥ B çº§ï¼Œç›®æ ‡ TARGET_COUNT æ¡"""
    all_news.sort(key=lambda x: x["score"], reverse=True)

    # å»é‡ï¼šåŒæ ‡é¢˜åªä¿ç•™æœ€é«˜åˆ†
    seen = set()
    unique = []
    for item in all_news:
        key = item["title"].lower().strip()
        if key not in seen:
            seen.add(key)
            unique.append(item)

    # å…ˆé€‰ S/A çº§
    selected = [n for n in unique if n["rating"] in ("S", "A")]

    # ä¸å¤Ÿåˆ™è¡¥ B çº§
    if len(selected) < TARGET_COUNT:
        b_pool = [n for n in unique if n["rating"] == "B"]
        selected.extend(b_pool[: TARGET_COUNT - len(selected)])

    # æœ€ç»ˆæŒ‰åˆ†æ•°æ’åº
    selected.sort(key=lambda x: x["score"], reverse=True)
    return selected


def generate_summary(news_list: list[dict]) -> str:
    """è®© LLM ç”Ÿæˆä»Šæ—¥æ€»ç»“æ¦‚è§ˆ"""
    headlines = "\n".join(
        [f"- [{n['rating']}|{n['score']}] {n['title']}" for n in news_list]
    )

    prompt = f"""ä½ æ˜¯ä¸€ä½ AI é¢†åŸŸæƒ…æŠ¥ç¼–è¾‘ã€‚ä»¥ä¸‹æ˜¯ä»Šå¤©ç­›é€‰å‡ºçš„é«˜ä»·å€¼ AI æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼š

{headlines}

è¯·ç”¨ä¸­æ–‡å†™ä¸€æ®µ 3-5 å¥è¯çš„ã€Œä»Šæ—¥æ¦‚è§ˆã€ï¼Œæ¦‚æ‹¬ä»Šå¤© AI é¢†åŸŸçš„æ•´ä½“åŠ¨æ€å’Œæœ€å€¼å¾—å…³æ³¨çš„æ–¹å‘ã€‚
è¦æ±‚ï¼šè¯­è¨€ç®€æ´æœ‰æ´å¯ŸåŠ›ï¼Œåƒä¸€ä½èµ„æ·±ç¼–è¾‘å†™çš„æ™¨æŠ¥å¯¼è¯­ï¼Œä¸è¦ç”¨ Markdown æ ¼å¼ã€‚"""

    result = call_qwen(prompt)
    return result.strip() if result else "ä»Šæ—¥ AI é¢†åŸŸåŠ¨æ€æ±‡æ€»å¦‚ä¸‹ã€‚"


def build_report(news_list: list[dict], summary: str) -> str:
    """æ„å»º Telegram HTML æ ¼å¼æ—¥æŠ¥ï¼šæ€»ç»“åœ¨å‰ï¼Œé€æ¡åœ¨å"""
    bj_time = datetime.now(timezone(timedelta(hours=8)))
    today = bj_time.strftime("%Y-%m-%d")

    lines = []
    # ===== æ ‡é¢˜ =====
    lines.append(f"<b>ğŸ“¡ AI æ¯æ—¥æƒ…æŠ¥ | {today}</b>")
    lines.append("")

    # ===== ä»Šæ—¥æ¦‚è§ˆ =====
    lines.append("<b>ğŸ§­ ä»Šæ—¥æ¦‚è§ˆ</b>")
    lines.append(f"<i>{summary}</i>")
    lines.append("")
    lines.append(f"å…±ç­›é€‰ <b>{len(news_list)}</b> æ¡å€¼å¾—å…³æ³¨çš„å†…å®¹ï¼š")
    lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    lines.append("")

    # ===== é€æ¡å±•ç¤º =====
    for i, item in enumerate(news_list, 1):
        # è¯„çº§ icon
        if item["rating"] == "S":
            badge = "ğŸ”´ S"
        elif item["rating"] == "A":
            badge = "ğŸŸ  A"
        else:
            badge = "ğŸŸ¡ B"

        tags = " ".join([f"#{t}" for t in item["tags"]]) if item["tags"] else ""

        lines.append(f"<b>{i}. {item['title']}</b>")
        lines.append(f"   {badge} Â· {item['score']}åˆ† Â· {item['source']}")
        lines.append(f"   ğŸ’¬ {item['comment']}")
        if tags:
            lines.append(f"   {tags}")
        lines.append(f"   ğŸ”— <a href='{item['link']}'>é˜…è¯»åŸæ–‡</a>")
        lines.append("")

    # ===== å°¾éƒ¨ =====
    lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    lines.append("<i>ğŸ¤– ç”± AI æƒ…æŠ¥ Agent è‡ªåŠ¨ç”Ÿæˆ</i>")

    return "\n".join(lines)


def send_telegram(text: str):
    """å‘é€æ¶ˆæ¯åˆ° Telegramï¼ŒæŒ‰æ®µè½æ™ºèƒ½åˆ†å‰²"""
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    max_len = 3800

    # æŒ‰ç©ºè¡Œåˆ†æ®µï¼Œå°½é‡ä¸åœ¨æ¡ç›®ä¸­é—´æˆªæ–­
    paragraphs = text.split("\n\n")
    chunks = []
    current = ""

    for para in paragraphs:
        if len(current) + len(para) + 2 > max_len:
            if current:
                chunks.append(current)
            current = para
        else:
            current = current + "\n\n" + para if current else para

    if current:
        chunks.append(current)

    for chunk in chunks:
        payload = {
            "chat_id": TELEGRAM_CHAT_ID,
            "text": chunk,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        }
        try:
            resp = requests.post(url, json=payload, timeout=30)
            if resp.status_code != 200:
                print(f"[Telegram å‘é€å¤±è´¥] {resp.text}")
        except Exception as e:
            print(f"[Telegram ç½‘ç»œé”™è¯¯] {e}")


def main():
    print("=== AI æ¯æ—¥æƒ…æŠ¥ Agent å¯åŠ¨ ===\n")
    all_news = []
    for src in SOURCES:
        all_news.extend(fetch_source(src))

    print(f"\nå…±æŠ“å–å¹¶è¯„çº§ {len(all_news)} æ¡å†…å®¹")

    # ç­›é€‰ top N
    selected = select_top_news(all_news)
    if not selected:
        print("ä»Šæ—¥æ— å€¼å¾—æ¨é€çš„å†…å®¹ã€‚")
        return

    print(f"ç­›é€‰å‡º {len(selected)} æ¡æ¨é€å†…å®¹ï¼Œæ­£åœ¨ç”Ÿæˆæ€»ç»“ ...")

    # LLM ç”Ÿæˆä»Šæ—¥æ€»ç»“
    summary = generate_summary(selected)
    print(f"ä»Šæ—¥æ¦‚è§ˆ: {summary}\n")

    # æ„å»ºå¹¶å‘é€æŠ¥å‘Š
    report = build_report(selected, summary)
    print("æ­£åœ¨æ¨é€ Telegram ...")
    send_telegram(report)
    print("æ¨é€å®Œæˆã€‚")


if __name__ == "__main__":
    main()

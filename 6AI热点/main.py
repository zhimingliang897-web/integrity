import os
import json
import html
import requests
import feedparser
from openai import OpenAI
from datetime import datetime, timezone, timedelta
from pathlib import Path

# --- é…ç½® ---
QWEN_API_KEY = os.getenv("QWEN_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "-1003878273027")
MODEL_NAME = "qwen3-vl-flash-2026-01-22"
TARGET_COUNT = 6  # ç›®æ ‡æ¨é€æ¡æ•°

BJ_TZ = timezone(timedelta(hours=8))
DOCS_DIR = Path(__file__).resolve().parent.parent / "docs"

client = OpenAI(
    api_key=QWEN_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# --- ä¿¡æºåˆ—è¡¨ ---
SOURCES = [
    # ===== å›½é™…ç¡¬æ ¸ =====
    {"name": "Hacker News", "url": "https://hnrss.org/newest?q=AI+LLM+Transformer+GPT", "type": "general", "count": 8},
    {"name": "HF Papers", "url": "https://rsshub.app/huggingface/daily-papers", "type": "general", "count": 8},
    {"name": "GitHub Trending", "url": "https://rsshub.app/github/trending/daily/python?since=daily", "type": "general", "count": 6},
    {"name": "ArXiv cs.AI", "url": "https://rss.arxiv.org/rss/cs.AI", "type": "general", "count": 6},
    {"name": "ArXiv cs.CL", "url": "https://rss.arxiv.org/rss/cs.CL", "type": "general", "count": 6},
    # ===== å›½é™…ç¤¾åŒº =====
    {"name": "Reddit LocalLLaMA", "url": "https://www.reddit.com/r/LocalLLaMA/.rss", "type": "general", "count": 6},
    {"name": "Reddit ML", "url": "https://www.reddit.com/r/MachineLearning/.rss", "type": "general", "count": 6},
    {"name": "OpenAI Blog", "url": "https://rsshub.app/openai/blog", "type": "general", "count": 5},
    {"name": "Google AI", "url": "https://rsshub.app/google/research", "type": "general", "count": 5},
    {"name": "MIT Tech Review", "url": "https://www.technologyreview.com/feed/", "type": "general", "count": 5},
    # ===== å›½å†…æƒå¨ =====
    {"name": "æœºå™¨ä¹‹å¿ƒ", "url": "https://www.jiqizhixin.com/rss", "type": "cn_media", "count": 6},
    {"name": "é‡å­ä½", "url": "https://rsshub.app/qbitai/category/èµ„è®¯", "type": "cn_media", "count": 6},
]


# ============================================================
# LLM è°ƒç”¨
# ============================================================

def call_qwen(prompt: str) -> str | None:
    try:
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
        )
        return resp.choices[0].message.content
    except Exception as e:
        print(f"[Qwen API Error] {e}")
        return None


def analyze_news(title: str, summary: str, source_type: str) -> dict | None:
    anti_hype = ""
    if source_type == "cn_media":
        anti_hype = (
            "æ³¨æ„ï¼šæœ¬æ–‡æ¥è‡ªä¸­æ–‡è‡ªåª’ä½“ï¼Œè¯·é€‚å½“å¿½ç•¥'ç‚¸è£‚''é¢ è¦†''ç¥ä½œ'ç­‰è¥é”€è¯æ±‡ï¼Œ"
            "ä¾§é‡å…³æ³¨æ˜¯å¦æœ‰ä»£ç /è®ºæ–‡/å®è´¨æŠ€æœ¯å†…å®¹ã€‚ä½†å¦‚æœåº•å±‚å†…å®¹ç¡®å®æœ‰ä»·å€¼ï¼Œä¸è¦å› ä¸ºæ ‡é¢˜å¤¸å¼ å°±é™åˆ†ã€‚"
        )

    prompt = f"""ä½ æ˜¯ä¸€ä½ AI æŠ€æœ¯æƒ…æŠ¥åˆ†æå¸ˆï¼Œè´Ÿè´£ä¸ºæŠ€æœ¯ä»ä¸šè€…ç­›é€‰æ¯æ—¥å€¼å¾—å…³æ³¨çš„å†…å®¹ã€‚
{anti_hype}

è¯·åˆ†æä»¥ä¸‹æ–°é—»ï¼š
æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{summary}

è¯„çº§æ ‡å‡†ï¼ˆè¯·åˆç†ç»™åˆ†ï¼Œä¸è¦è¿‡äºè‹›åˆ»ï¼‰ï¼š
- Sçº§ (9-10): è¡Œä¸šé‡Œç¨‹ç¢‘äº‹ä»¶ã€é¢ è¦†æ€§æ¶æ„çªç ´ã€é‡é‡çº§äº§å“å‘å¸ƒã€‚
- Açº§ (7-8): é«˜è´¨é‡è®ºæ–‡ã€å®ç”¨å·¥å…·é‡å¤§æ›´æ–°ã€å¤§å‚é‡è¦å¼€æºã€å€¼å¾—å…³æ³¨çš„è¡Œä¸šåŠ¨æ€ã€‚
- Bçº§ (5-6): æœ‰ä¸€å®šå‚è€ƒä»·å€¼çš„å†…å®¹ã€ä¸­ç­‰è´¨é‡çš„æŠ€æœ¯åˆ†äº«ã€æ™®é€šäº§å“è¿­ä»£ã€‚
- Cçº§ (1-4): çº¯æ°´æ–‡ã€æ— å®è´¨å†…å®¹çš„å…¬å…³ç¨¿ã€é‡å¤æ—§é—»ã€‚

è¯·åªè¾“å‡ºä¸€ä¸ªåˆæ³• JSONï¼Œä¸è¦åŒ…å« Markdown ä»£ç å—æ ‡è®°ï¼š
{{"rating":"S/A/B/C","score":æ•´æ•°1åˆ°10,"comment":"ä¸€å¥è¯ç‚¹è¯„(ä¸­æ–‡,20å­—å†…)","tags":["æ ‡ç­¾1","æ ‡ç­¾2"]}}"""

    text = call_qwen(prompt)
    if not text:
        return None

    clean = text.replace("```json", "").replace("```", "").strip()
    start = clean.find("{")
    end = clean.rfind("}") + 1
    if start == -1 or end == 0:
        print(f"[JSON æå–å¤±è´¥] {title}")
        return None
    try:
        return json.loads(clean[start:end])
    except json.JSONDecodeError:
        print(f"[JSON è§£æå¤±è´¥] {title}")
        return None


# ============================================================
# æ•°æ®é‡‡é›†
# ============================================================

def fetch_source(source: dict) -> list[dict]:
    name = source["name"]
    count = source.get("count", 5)
    print(f">> æ­£åœ¨æŠ“å–: {name} ...")
    try:
        feed = feedparser.parse(source["url"])
    except Exception as e:
        print(f"[RSS æŠ“å–å¤±è´¥] {name}: {e}")
        return []

    if not feed.entries:
        print(f"   (æ— å†…å®¹)")
        return []

    results = []
    for entry in feed.entries[:count]:
        title = getattr(entry, "title", "").strip()
        summary = getattr(entry, "summary", "")[:800]
        link = getattr(entry, "link", "")
        if not title:
            continue

        analysis = analyze_news(title, summary, source["type"])
        if not analysis:
            continue

        rating = analysis.get("rating", "C")
        score = analysis.get("score", 0)
        results.append({
            "source": name,
            "title": title,
            "link": link,
            "rating": rating,
            "score": score,
            "comment": analysis.get("comment", ""),
            "tags": analysis.get("tags", []),
        })
        print(f"   [{rating}|{score}] {title}")

    return results


def select_top_news(all_news: list[dict]) -> list[dict]:
    all_news.sort(key=lambda x: x["score"], reverse=True)

    seen = set()
    unique = []
    for item in all_news:
        key = item["title"].lower().strip()
        if key not in seen:
            seen.add(key)
            unique.append(item)

    selected = [n for n in unique if n["rating"] in ("S", "A")]

    if len(selected) < TARGET_COUNT:
        b_pool = [n for n in unique if n["rating"] == "B"]
        selected.extend(b_pool[: TARGET_COUNT - len(selected)])

    selected.sort(key=lambda x: x["score"], reverse=True)
    return selected


# ============================================================
# LLM ç”Ÿæˆæ€»ç»“
# ============================================================

def generate_summary(news_list: list[dict]) -> str:
    headlines = "\n".join(
        [f"- [{n['rating']}|{n['score']}] {n['title']}" for n in news_list]
    )
    prompt = f"""ä½ æ˜¯ä¸€ä½ AI é¢†åŸŸæƒ…æŠ¥ç¼–è¾‘ã€‚ä»¥ä¸‹æ˜¯ä»Šå¤©ç­›é€‰å‡ºçš„ AI æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼š

{headlines}

è¯·ç”¨ä¸­æ–‡å†™ 2-3 å¥è¯çš„ã€Œä»Šæ—¥æ¦‚è§ˆã€ï¼Œæ¦‚æ‹¬ä»Šå¤©æœ€å€¼å¾—å…³æ³¨çš„æ–¹å‘ã€‚
è¦æ±‚ï¼šç®€æ´ã€æœ‰æ´å¯ŸåŠ›ã€ä¸è¶…è¿‡ 80 å­—ï¼Œä¸è¦ç”¨ Markdown æ ¼å¼ã€‚"""

    result = call_qwen(prompt)
    return result.strip() if result else "ä»Šæ—¥ AI é¢†åŸŸåŠ¨æ€æ±‡æ€»å¦‚ä¸‹ã€‚"


# ============================================================
# Telegram æ¨é€
# ============================================================

def build_telegram_report(news_list: list[dict], summary: str) -> str:
    today = datetime.now(BJ_TZ).strftime("%Y-%m-%d")
    lines = []

    lines.append(f"<b>ğŸ“¡ AI æƒ…æŠ¥ | {today}</b>")
    lines.append(f"<i>{summary}</i>")
    lines.append("")

    for i, item in enumerate(news_list, 1):
        badge = {"S": "ğŸ”´S", "A": "ğŸŸ A"}.get(item["rating"], "ğŸŸ¡B")
        tags = " ".join([f"#{t}" for t in item["tags"]]) if item["tags"] else ""
        title_escaped = html.escape(item["title"])
        comment_escaped = html.escape(item["comment"])

        lines.append(f"<b>{i}. {title_escaped}</b>")
        lines.append(f"  {badge}Â·{item['score']}åˆ† | {item['source']}")
        lines.append(f"  {comment_escaped}")
        if tags:
            lines.append(f"  {tags}")
        lines.append(f"  <a href='{item['link']}'>åŸæ–‡</a>")
        lines.append("")

    return "\n".join(lines)


def send_telegram(text: str):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    max_len = 3800

    paragraphs = text.split("\n\n")
    chunks = []
    current = ""
    for para in paragraphs:
        if len(current) + len(para) + 2 > max_len:
            if current:
                chunks.append(current)
            current = para
        else:
            current = current + "\n\n" + para if current else para
    if current:
        chunks.append(current)

    for chunk in chunks:
        payload = {
            "chat_id": TELEGRAM_CHAT_ID,
            "text": chunk,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        }
        try:
            resp = requests.post(url, json=payload, timeout=30)
            if resp.status_code != 200:
                print(f"[Telegram å‘é€å¤±è´¥] {resp.text}")
        except Exception as e:
            print(f"[Telegram ç½‘ç»œé”™è¯¯] {e}")


# ============================================================
# GitHub Pages ç½‘é¡µç”Ÿæˆ
# ============================================================

def build_html_page(news_list: list[dict], summary: str) -> str:
    today = datetime.now(BJ_TZ).strftime("%Y-%m-%d")

    news_html = ""
    for item in news_list:
        badge_cls = {"S": "badge-s", "A": "badge-a"}.get(item["rating"], "badge-b")
        badge_text = item["rating"]
        tags = "".join([f'<span class="tag">{html.escape(t)}</span>' for t in item["tags"]])
        title_escaped = html.escape(item["title"])
        comment_escaped = html.escape(item["comment"])

        news_html += f"""
        <div class="card">
            <div class="card-header">
                <span class="badge {badge_cls}">{badge_text}</span>
                <span class="score">{item['score']}åˆ†</span>
                <span class="source">{html.escape(item['source'])}</span>
            </div>
            <h3 class="card-title"><a href="{item['link']}" target="_blank">{title_escaped}</a></h3>
            <p class="comment">{comment_escaped}</p>
            <div class="tags">{tags}</div>
        </div>"""

    return f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI æ¯æ—¥çƒ­ç‚¹ | Integrity Lab</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <nav>
        <div class="nav-inner">
            <span class="logo">Integrity Lab</span>
            <div class="nav-links">
                <a href="index.html">é¦–é¡µ</a>
                <a href="news.html" class="active">AI çƒ­ç‚¹</a>
                <a href="tools.html">å·¥å…·åº“</a>
            </div>
        </div>
    </nav>

    <div class="container">
        <div class="page-header">
            <h1>ğŸ“¡ AI æ¯æ—¥çƒ­ç‚¹</h1>
            <div class="date">{today} Â· å…± {len(news_list)} æ¡ç²¾é€‰ Â· æ¯æ—¥ 09:00 è‡ªåŠ¨æ›´æ–°</div>
        </div>
        <div class="summary">{html.escape(summary)}</div>
        {news_html}
    </div>

    <footer>Integrity Lab Â· ç”± AI æƒ…æŠ¥ Agent è‡ªåŠ¨ç”Ÿæˆ Â· GitHub Actions æ¯æ—¥æ›´æ–°</footer>
</body>
</html>"""


def save_html(news_list: list[dict], summary: str):
    DOCS_DIR.mkdir(parents=True, exist_ok=True)
    page = build_html_page(news_list, summary)
    (DOCS_DIR / "news.html").write_text(page, encoding="utf-8")
    print(f"ç½‘é¡µå·²ç”Ÿæˆ: {DOCS_DIR / 'news.html'}")


# ============================================================
# ä¸»æµç¨‹
# ============================================================

def main():
    print("=== AI æ¯æ—¥æƒ…æŠ¥ Agent å¯åŠ¨ ===\n")
    all_news = []
    for src in SOURCES:
        all_news.extend(fetch_source(src))

    print(f"\nå…±æŠ“å–å¹¶è¯„çº§ {len(all_news)} æ¡å†…å®¹")

    selected = select_top_news(all_news)
    if not selected:
        print("ä»Šæ—¥æ— å€¼å¾—æ¨é€çš„å†…å®¹ã€‚")
        return

    print(f"ç­›é€‰å‡º {len(selected)} æ¡æ¨é€å†…å®¹ï¼Œæ­£åœ¨ç”Ÿæˆæ€»ç»“ ...")

    summary = generate_summary(selected)
    print(f"ä»Šæ—¥æ¦‚è§ˆ: {summary}\n")

    # Telegram æ¨é€
    tg_report = build_telegram_report(selected, summary)
    print("æ­£åœ¨æ¨é€ Telegram ...")
    send_telegram(tg_report)
    print("Telegram æ¨é€å®Œæˆã€‚")

    # ç”Ÿæˆç½‘é¡µ
    save_html(selected, summary)
    print("å…¨éƒ¨å®Œæˆã€‚")


if __name__ == "__main__":
    main()
